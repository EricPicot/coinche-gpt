{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of game and reward + prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda/envs/gym-coinche/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import train_test_split as tts \n",
    "from tensorflow.keras.losses import mse, mean_absolute_error\n",
    "from tensorflow.keras.optimizers import RMSprop, Adadelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_formula(target, cols):\n",
    "    \n",
    "    f = target + \" ~ \"\n",
    "    f += \" + \".join(map(lambda x: \"C(\" + x +\")\" , cols))\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "def compute_pseudo_r_squared(mod):\n",
    "    return 1 - mod.deviance/mod.null_deviance\n",
    "\n",
    "\n",
    "def catch_card(xx):\n",
    "    try:\n",
    "        card = re.split(r'\\)',re.split(r'\\(',xx)[1])[0].split('_')\n",
    "    except:\n",
    "        card = [\"intercept\", \"intercept\", \"intercept\"]\n",
    "        \n",
    "    return card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results(model, color = 'rgb(158, 202, 225)'):\n",
    "    \n",
    "    results = model.conf_int()\n",
    "    results.columns = [\"coef_inf\",\"coef_sup\"]\n",
    "    results[\"coef \"]= model.params\n",
    "    results[\"p_value\"] = model.pvalues\n",
    "    results[\"color\"] = color\n",
    "    results[\"card\"] = results.index.map(catch_card)\n",
    "    results[[\"is_atout\", \"value\", \"player\"]] = results[\"card\"].apply(pd.Series)\n",
    "    \n",
    "    results[\"is_atout\"] = results[\"is_atout\"]==\"atout\"\n",
    "    del results[\"card\"]\n",
    "    return results\n",
    "\n",
    "def get_style_for_results(results):\n",
    "    \"\"\"\n",
    "    get style to highlight the significant values of a model's results \n",
    "    \n",
    "    Args :\n",
    "        df (dataframe) : model'results\n",
    "            \n",
    "    Returns : (df) : return a df with the styles to apply\n",
    "    \"\"\"\n",
    "    c1 = 'background-color: #80E37C'\n",
    "    c2 = '' \n",
    "    \n",
    "    #condition to be green\n",
    "    mask = results['p_value'] < 0.05\n",
    "    \n",
    "    #DataFrame with same index and columns names as original filled red color\n",
    "    df1 =  pd.DataFrame(c2, index=results.index, columns=results.columns)\n",
    "    \n",
    "    #modify values of df1 column by boolean mask\n",
    "    df1.loc[mask, [\"coef_inf\",\"coef_sup\"]] = c1\n",
    "    return df1\n",
    "\n",
    "\n",
    "def get_results_formated(model, caption = \"\", color= \"rgb(158,202,225)\"):\n",
    "    \"\"\"\n",
    "    highlights the significant values of a model's results \n",
    "    \n",
    "    Args :\n",
    "        model (model) : fitted model\n",
    "        caption (str) : text describing the results (optional)\n",
    "            \n",
    "    Returns : (df with style) : return a stylized df\n",
    "    \"\"\"\n",
    "    results = get_results(model, color = color)\n",
    "    \n",
    "    return results.style.apply(get_style_for_results, axis=None)\\\n",
    "           .set_caption(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/hands_and_reward.csv')\n",
    "data = data.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "\n",
    "suit = [\"atout\", \"na1\", \"na2\", \"na3\"]\n",
    "cards = [\"7\", \"8\", \"9\", \"10\", \"jack\", \"queen\", \"king\", \"as\"]\n",
    "players = [\"p1\", \"p2\"]\n",
    "data.columns = [(suit*2)[x//8]+\"_\"+cards[x%8]+\"_\"+players[x//32] for x in range(64)] + [\"total_reward\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atout_7_p1</th>\n",
       "      <th>atout_8_p1</th>\n",
       "      <th>atout_9_p1</th>\n",
       "      <th>atout_10_p1</th>\n",
       "      <th>atout_jack_p1</th>\n",
       "      <th>atout_queen_p1</th>\n",
       "      <th>atout_king_p1</th>\n",
       "      <th>atout_as_p1</th>\n",
       "      <th>na1_7_p1</th>\n",
       "      <th>na1_8_p1</th>\n",
       "      <th>...</th>\n",
       "      <th>na2_as_p2</th>\n",
       "      <th>na3_7_p2</th>\n",
       "      <th>na3_8_p2</th>\n",
       "      <th>na3_9_p2</th>\n",
       "      <th>na3_10_p2</th>\n",
       "      <th>na3_jack_p2</th>\n",
       "      <th>na3_queen_p2</th>\n",
       "      <th>na3_king_p2</th>\n",
       "      <th>na3_as_p2</th>\n",
       "      <th>total_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   atout_7_p1  atout_8_p1  atout_9_p1  atout_10_p1  atout_jack_p1  \\\n",
       "0         0.0         0.0         0.0          0.0            1.0   \n",
       "1         0.0         0.0         0.0          0.0            0.0   \n",
       "2         0.0         1.0         0.0          1.0            0.0   \n",
       "3         1.0         0.0         0.0          0.0            1.0   \n",
       "4         0.0         1.0         1.0          0.0            0.0   \n",
       "\n",
       "   atout_queen_p1  atout_king_p1  atout_as_p1  na1_7_p1  na1_8_p1  ...  \\\n",
       "0             1.0            1.0          0.0       0.0       0.0  ...   \n",
       "1             0.0            1.0          0.0       0.0       1.0  ...   \n",
       "2             0.0            1.0          0.0       0.0       0.0  ...   \n",
       "3             0.0            0.0          0.0       0.0       0.0  ...   \n",
       "4             0.0            1.0          0.0       1.0       0.0  ...   \n",
       "\n",
       "   na2_as_p2  na3_7_p2  na3_8_p2  na3_9_p2  na3_10_p2  na3_jack_p2  \\\n",
       "0        0.0       0.0       0.0       0.0        0.0          0.0   \n",
       "1        0.0       1.0       0.0       1.0        0.0          0.0   \n",
       "2        1.0       0.0       0.0       1.0        0.0          1.0   \n",
       "3        0.0       1.0       0.0       1.0        0.0          1.0   \n",
       "4        1.0       0.0       0.0       0.0        0.0          1.0   \n",
       "\n",
       "   na3_queen_p2  na3_king_p2  na3_as_p2  total_reward  \n",
       "0           0.0          1.0        1.0         111.0  \n",
       "1           0.0          0.0        1.0          32.0  \n",
       "2           0.0          0.0        0.0          68.0  \n",
       "3           0.0          0.0        1.0         101.0  \n",
       "4           1.0          0.0        0.0          64.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check that the data is centered in 81, and in the intervale [0, 162]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    100000.000000\n",
       "mean         81.170100\n",
       "std          35.247125\n",
       "min           0.000000\n",
       "25%          55.000000\n",
       "50%          81.000000\n",
       "75%         107.000000\n",
       "max         162.000000\n",
       "Name: total_reward, dtype: float64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.hist(data[\"total_reward\"], bins=np.arange(10, 170, 5), label=\"3 random + 1 AI\")\n",
    "plt.title(\" Impact of sampling on random total reward\")\n",
    "plt.show()\n",
    "\n",
    "data[\"total_reward\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll start by training a linear model, to check if card have a value that is linked to its rank and its suit:**\n",
    "\n",
    "We suppose that all the cards are independants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total_reward ~ C(atout_7_p1) + C(atout_8_p1) + C(atout_9_p1) + C(atout_10_p1) + C(atout_jack_p1) + C(atout_queen_p1) + C(atout_king_p1) + C(atout_as_p1) + C(na1_7_p1) + C(na1_8_p1) + C(na1_9_p1) + C(na1_10_p1) + C(na1_jack_p1) + C(na1_queen_p1) + C(na1_king_p1) + C(na1_as_p1) + C(na2_7_p1) + C(na2_8_p1) + C(na2_9_p1) + C(na2_10_p1) + C(na2_jack_p1) + C(na2_queen_p1) + C(na2_king_p1) + C(na2_as_p1) + C(na3_7_p1) + C(na3_8_p1) + C(na3_9_p1) + C(na3_10_p1) + C(na3_jack_p1) + C(na3_queen_p1) + C(na3_king_p1) + C(na3_as_p1) + C(atout_7_p2) + C(atout_8_p2) + C(atout_9_p2) + C(atout_10_p2) + C(atout_jack_p2) + C(atout_queen_p2) + C(atout_king_p2) + C(atout_as_p2) + C(na1_7_p2) + C(na1_8_p2) + C(na1_9_p2) + C(na1_10_p2) + C(na1_jack_p2) + C(na1_queen_p2) + C(na1_king_p2) + C(na1_as_p2) + C(na2_7_p2) + C(na2_8_p2) + C(na2_9_p2) + C(na2_10_p2) + C(na2_jack_p2) + C(na2_queen_p2) + C(na2_king_p2) + C(na2_as_p2) + C(na3_7_p2) + C(na3_8_p2) + C(na3_9_p2) + C(na3_10_p2) + C(na3_jack_p2) + C(na3_queen_p2) + C(na3_king_p2) + C(na3_as_p2)'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula = build_formula(\"total_reward\", cols = np.concatenate([data.columns[:-1]]).tolist())\n",
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create linear model and get first results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod = smf.glm(formula=formula, data=data, family=sm.families.Gaussian()).fit()\n",
    "print(\"pseudo_R2: \", compute_pseudo_r_squared(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "formated_results = get_results_formated(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As game and playing are random and as non atout suits are playing a symetrical role, we can average the coef value directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_coef = formated_results.data.groupby([\"is_atout\", \"value\"]).agg({\"p_value\": \"max\",\n",
    "                                                                         \"coef_inf\":\"mean\",\n",
    "                                                                         \"coef_sup\":\"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(average_coef.style\n",
    "            .apply(get_style_for_results, axis=None)\n",
    "            .set_caption(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First neural network model with convolutions taking two players games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creatgin class_weigth dict to penalize extreme values\n",
    "# thresholds = [30, 50, 80, 100, 110]\n",
    "class_weights = {}\n",
    "for i in range(163):\n",
    "    i = float(i)\n",
    "    if i > 110:\n",
    "        class_weights[i] = 4\n",
    "    elif (i <= 50) | (i > 100):\n",
    "        class_weights[i] = 3\n",
    "    elif (i > 82) | (i <60):\n",
    "        class_weights[i] = 2\n",
    "    else:\n",
    "        class_weights[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=16, kernel_size=4, activation='relu', input_shape=(64, 1)))\n",
    "    model.add(layers.AveragePooling1D(pool_size = 4, strides=4))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv1D(filters=8, kernel_size=4, activation='relu'))\n",
    "    model.add(layers.AveragePooling1D(pool_size = 4))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv1D(filters=4, kernel_size=2, activation='relu'))\n",
    "    model.add(layers.AveragePooling1D(pool_size = 2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(8, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu', input_shape=(64,)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(8, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "loss = mse\n",
    "\n",
    "model = build_cnn_model()\n",
    "model.compile(optimizer=\"adam\", loss=loss)\n",
    "\n",
    "model_class_weight = build_cnn_model()\n",
    "model_class_weight.compile(optimizer=\"adam\", loss=loss)\n",
    "\n",
    "relu_class_weigth_model = build_model()\n",
    "relu_class_weigth_model.compile(optimizer=\"adam\", loss=loss)\n",
    "\n",
    "relu_model = build_model()\n",
    "relu_model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = tts(data[data.columns[:-1]].values, data.total_reward, test_size=0.2, random_state=4)\n",
    "xtrain = xtrain.reshape((xtrain.shape[0], xtrain.shape[1], 1))\n",
    "xtest = xtest.reshape((xtest.shape[0], xtest.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "80000/80000 [==============================] - 7s 84us/sample - loss: 2240.5947 - val_loss: 1298.6195\n",
      "Epoch 2/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 1035.6216 - val_loss: 1110.3032\n",
      "Epoch 3/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 963.9361 - val_loss: 968.6972\n",
      "Epoch 4/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 922.3071 - val_loss: 916.7928\n",
      "Epoch 5/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 888.5508 - val_loss: 873.9589\n",
      "Epoch 6/50\n",
      "80000/80000 [==============================] - 4s 45us/sample - loss: 867.6814 - val_loss: 861.2080\n",
      "Epoch 7/50\n",
      "80000/80000 [==============================] - 4s 45us/sample - loss: 854.6988 - val_loss: 849.4761\n",
      "Epoch 8/50\n",
      "80000/80000 [==============================] - 4s 53us/sample - loss: 846.1947 - val_loss: 856.1860\n",
      "Epoch 9/50\n",
      "80000/80000 [==============================] - 5s 57us/sample - loss: 837.5393 - val_loss: 834.6463\n",
      "Epoch 10/50\n",
      "80000/80000 [==============================] - 4s 53us/sample - loss: 830.9598 - val_loss: 815.0605\n",
      "Epoch 11/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 821.4871 - val_loss: 865.6926\n",
      "Epoch 12/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 818.2607 - val_loss: 850.0987\n",
      "Epoch 13/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 810.7876 - val_loss: 846.0379\n",
      "Epoch 14/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 807.1528 - val_loss: 789.1952\n",
      "Epoch 15/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 798.9693 - val_loss: 844.3492\n",
      "Epoch 16/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 796.1485 - val_loss: 910.3857\n",
      "Epoch 17/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 791.8240 - val_loss: 812.7623\n",
      "Epoch 18/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 787.9138 - val_loss: 792.6371\n",
      "Epoch 19/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 784.9026 - val_loss: 781.1495\n",
      "Epoch 20/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 783.0325 - val_loss: 796.4836\n",
      "Epoch 21/50\n",
      "80000/80000 [==============================] - 4s 45us/sample - loss: 781.2586 - val_loss: 861.4106\n",
      "Epoch 22/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 778.1268 - val_loss: 805.5262\n",
      "Epoch 23/50\n",
      "80000/80000 [==============================] - 4s 54us/sample - loss: 777.5640 - val_loss: 772.6979\n",
      "Epoch 24/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 773.5057 - val_loss: 852.8788\n",
      "Epoch 25/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 772.6106 - val_loss: 777.9727\n",
      "Epoch 26/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 771.1453 - val_loss: 770.5254\n",
      "Epoch 27/50\n",
      "80000/80000 [==============================] - 4s 45us/sample - loss: 766.2529 - val_loss: 757.9038\n",
      "Epoch 28/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 762.7166 - val_loss: 765.4411\n",
      "Epoch 29/50\n",
      "80000/80000 [==============================] - 4s 53us/sample - loss: 762.7831 - val_loss: 830.5123\n",
      "Epoch 30/50\n",
      "80000/80000 [==============================] - 4s 51us/sample - loss: 760.5601 - val_loss: 764.9168\n",
      "Epoch 31/50\n",
      "80000/80000 [==============================] - 4s 45us/sample - loss: 759.2688 - val_loss: 755.5677\n",
      "Epoch 32/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 756.2929 - val_loss: 748.3161\n",
      "Epoch 33/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 753.9844 - val_loss: 746.6111\n",
      "Epoch 34/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 753.9511 - val_loss: 806.3776\n",
      "Epoch 35/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 750.5511 - val_loss: 868.8498\n",
      "Epoch 36/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 749.2153 - val_loss: 745.1237\n",
      "Epoch 37/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 744.5598 - val_loss: 787.6835\n",
      "Epoch 38/50\n",
      "80000/80000 [==============================] - 4s 53us/sample - loss: 743.0109 - val_loss: 737.9652\n",
      "Epoch 39/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 741.5441 - val_loss: 794.6032\n",
      "Epoch 40/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 738.2743 - val_loss: 745.3286\n",
      "Epoch 41/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 735.2463 - val_loss: 737.6836\n",
      "Epoch 42/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 735.4667 - val_loss: 750.6146\n",
      "Epoch 43/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 731.7798 - val_loss: 721.3837\n",
      "Epoch 44/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 729.6721 - val_loss: 753.5172\n",
      "Epoch 45/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 731.3978 - val_loss: 719.7862\n",
      "Epoch 46/50\n",
      "80000/80000 [==============================] - 4s 45us/sample - loss: 730.2977 - val_loss: 731.6690\n",
      "Epoch 47/50\n",
      "80000/80000 [==============================] - 4s 45us/sample - loss: 729.3398 - val_loss: 787.7472\n",
      "Epoch 48/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 726.0333 - val_loss: 766.6014\n",
      "Epoch 49/50\n",
      "80000/80000 [==============================] - 4s 45us/sample - loss: 724.4335 - val_loss: 713.1256\n",
      "Epoch 50/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 728.2344 - val_loss: 715.4883\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "80000/80000 [==============================] - 6s 72us/sample - loss: 6965.5548 - val_loss: 1180.1885\n",
      "Epoch 2/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 3114.1883 - val_loss: 982.7099\n",
      "Epoch 3/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 2808.4545 - val_loss: 974.1509\n",
      "Epoch 4/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 2568.0266 - val_loss: 1498.6753\n",
      "Epoch 5/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 2420.7526 - val_loss: 1013.2784\n",
      "Epoch 6/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 2359.8670 - val_loss: 1116.2876\n",
      "Epoch 7/50\n",
      "80000/80000 [==============================] - 4s 56us/sample - loss: 2334.8426 - val_loss: 828.3578\n",
      "Epoch 8/50\n",
      "80000/80000 [==============================] - 4s 53us/sample - loss: 2298.9631 - val_loss: 849.8552\n",
      "Epoch 9/50\n",
      "80000/80000 [==============================] - 4s 49us/sample - loss: 2267.8044 - val_loss: 863.1971\n",
      "Epoch 10/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 2251.6865 - val_loss: 1163.8983\n",
      "Epoch 11/50\n",
      "80000/80000 [==============================] - 4s 49us/sample - loss: 2230.2930 - val_loss: 879.8417\n",
      "Epoch 12/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 2211.8920 - val_loss: 840.5670\n",
      "Epoch 13/50\n",
      "80000/80000 [==============================] - 4s 49us/sample - loss: 2195.9557 - val_loss: 808.6109\n",
      "Epoch 14/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 2182.0592 - val_loss: 779.4286\n",
      "Epoch 15/50\n",
      "80000/80000 [==============================] - 5s 57us/sample - loss: 2167.7325 - val_loss: 826.0652\n",
      "Epoch 16/50\n",
      "80000/80000 [==============================] - 5s 60us/sample - loss: 2163.5039 - val_loss: 816.4123\n",
      "Epoch 17/50\n",
      "80000/80000 [==============================] - 4s 51us/sample - loss: 2152.0681 - val_loss: 884.2275\n",
      "Epoch 18/50\n",
      "80000/80000 [==============================] - 4s 49us/sample - loss: 2141.2364 - val_loss: 881.0798\n",
      "Epoch 19/50\n",
      "80000/80000 [==============================] - 4s 55us/sample - loss: 2132.3282 - val_loss: 804.8803\n",
      "Epoch 20/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 2125.3007 - val_loss: 766.9167\n",
      "Epoch 21/50\n",
      "80000/80000 [==============================] - 4s 50us/sample - loss: 2123.0750 - val_loss: 778.4059\n",
      "Epoch 22/50\n",
      "80000/80000 [==============================] - 4s 51us/sample - loss: 2116.3876 - val_loss: 956.7505\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 4s 48us/sample - loss: 2114.1343 - val_loss: 855.3693\n",
      "Epoch 24/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 2102.1229 - val_loss: 871.7902\n",
      "Epoch 25/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 2102.3861 - val_loss: 786.0623\n",
      "Epoch 26/50\n",
      "80000/80000 [==============================] - 4s 48us/sample - loss: 2102.3413 - val_loss: 1077.5386\n",
      "Epoch 27/50\n",
      "80000/80000 [==============================] - 4s 49us/sample - loss: 2095.1045 - val_loss: 1844.5759\n",
      "Epoch 28/50\n",
      "80000/80000 [==============================] - 4s 46us/sample - loss: 2088.2904 - val_loss: 785.2024\n",
      "Epoch 29/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 2094.8655 - val_loss: 762.6303\n",
      "Epoch 30/50\n",
      "80000/80000 [==============================] - 4s 47us/sample - loss: 2084.7148 - val_loss: 754.7440\n",
      "Epoch 31/50\n",
      "80000/80000 [==============================] - 4s 53us/sample - loss: 2081.2614 - val_loss: 781.6079\n",
      "Epoch 32/50\n",
      "80000/80000 [==============================] - 5s 62us/sample - loss: 2080.0869 - val_loss: 807.4572\n",
      "Epoch 33/50\n",
      "80000/80000 [==============================] - 6s 70us/sample - loss: 2082.6147 - val_loss: 799.6479\n",
      "Epoch 34/50\n",
      "80000/80000 [==============================] - 5s 60us/sample - loss: 2077.4852 - val_loss: 918.4567\n",
      "Epoch 35/50\n",
      "80000/80000 [==============================] - 5s 57us/sample - loss: 2075.2456 - val_loss: 1048.3759\n",
      "Epoch 36/50\n",
      "80000/80000 [==============================] - 4s 55us/sample - loss: 2066.1760 - val_loss: 774.0465\n",
      "Epoch 37/50\n",
      "80000/80000 [==============================] - 5s 64us/sample - loss: 2067.7229 - val_loss: 841.2332\n",
      "Epoch 38/50\n",
      "80000/80000 [==============================] - 5s 62us/sample - loss: 2066.8482 - val_loss: 1044.0069\n",
      "Epoch 39/50\n",
      "80000/80000 [==============================] - 5s 60us/sample - loss: 2061.3852 - val_loss: 740.5731\n",
      "Epoch 40/50\n",
      "80000/80000 [==============================] - 5s 59us/sample - loss: 2057.1596 - val_loss: 1004.8599\n",
      "Epoch 41/50\n",
      "80000/80000 [==============================] - 5s 66us/sample - loss: 2051.7799 - val_loss: 751.2006\n",
      "Epoch 42/50\n",
      "80000/80000 [==============================] - 5s 65us/sample - loss: 2051.8709 - val_loss: 824.6769\n",
      "Epoch 43/50\n",
      "80000/80000 [==============================] - 6s 75us/sample - loss: 2048.6701 - val_loss: 869.5576\n",
      "Epoch 44/50\n",
      "80000/80000 [==============================] - 6s 71us/sample - loss: 2050.7740 - val_loss: 795.5315\n",
      "Epoch 45/50\n",
      "80000/80000 [==============================] - 5s 68us/sample - loss: 2044.4652 - val_loss: 739.2317\n",
      "Epoch 46/50\n",
      "80000/80000 [==============================] - 4s 53us/sample - loss: 2038.2588 - val_loss: 884.1742\n",
      "Epoch 47/50\n",
      "80000/80000 [==============================] - 4s 54us/sample - loss: 2038.6902 - val_loss: 758.4307\n",
      "Epoch 48/50\n",
      "80000/80000 [==============================] - 4s 53us/sample - loss: 2044.0766 - val_loss: 754.9164\n",
      "Epoch 49/50\n",
      "80000/80000 [==============================] - 4s 56us/sample - loss: 2035.5631 - val_loss: 901.2215\n",
      "Epoch 50/50\n",
      "80000/80000 [==============================] - 4s 51us/sample - loss: 2026.8387 - val_loss: 857.3710\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "80000/80000 [==============================] - 3s 37us/sample - loss: 1665.6390 - val_loss: 599.2523\n",
      "Epoch 2/50\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 589.2758 - val_loss: 587.1210\n",
      "Epoch 3/50\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 578.2088 - val_loss: 575.3931\n",
      "Epoch 4/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 571.7651 - val_loss: 573.5689\n",
      "Epoch 5/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 565.9597 - val_loss: 565.3804\n",
      "Epoch 6/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 561.4188 - val_loss: 566.2506\n",
      "Epoch 7/50\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 558.8362 - val_loss: 564.9335\n",
      "Epoch 8/50\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 557.5421 - val_loss: 565.4950\n",
      "Epoch 9/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 554.6247 - val_loss: 565.5521\n",
      "Epoch 10/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 552.6748 - val_loss: 568.7240\n",
      "Epoch 11/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 551.2109 - val_loss: 567.7907\n",
      "Epoch 12/50\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 549.8327 - val_loss: 568.3016\n",
      "Epoch 13/50\n",
      "80000/80000 [==============================] - 1s 19us/sample - loss: 548.6831 - val_loss: 572.7406\n",
      "Epoch 14/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 546.9794 - val_loss: 570.1324\n",
      "Epoch 15/50\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 544.9154 - val_loss: 570.9686\n",
      "Epoch 16/50\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 543.3885 - val_loss: 572.3177\n",
      "Epoch 17/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 544.1845 - val_loss: 572.5932\n",
      "Epoch 18/50\n",
      "80000/80000 [==============================] - 2s 21us/sample - loss: 542.1803 - val_loss: 573.8126\n",
      "Epoch 19/50\n",
      "80000/80000 [==============================] - 3s 38us/sample - loss: 541.9583 - val_loss: 595.6374\n",
      "Epoch 20/50\n",
      "80000/80000 [==============================] - 2s 25us/sample - loss: 540.5558 - val_loss: 574.7988\n",
      "Epoch 21/50\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 539.0491 - val_loss: 581.0881\n",
      "Epoch 22/50\n",
      "80000/80000 [==============================] - 2s 23us/sample - loss: 537.7360 - val_loss: 573.8700\n",
      "Epoch 23/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 536.1716 - val_loss: 575.7697\n",
      "Epoch 24/50\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 536.0410 - val_loss: 576.4393\n",
      "Epoch 25/50\n",
      "80000/80000 [==============================] - 2s 21us/sample - loss: 534.0035 - val_loss: 577.7253\n",
      "Epoch 26/50\n",
      "80000/80000 [==============================] - 2s 21us/sample - loss: 533.3356 - val_loss: 581.4807\n",
      "Epoch 27/50\n",
      "80000/80000 [==============================] - 2s 22us/sample - loss: 531.1691 - val_loss: 578.7399\n",
      "Epoch 28/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 531.3435 - val_loss: 590.8195\n",
      "Epoch 29/50\n",
      "80000/80000 [==============================] - 2s 21us/sample - loss: 528.8731 - val_loss: 583.1826\n",
      "Epoch 30/50\n",
      "80000/80000 [==============================] - 2s 21us/sample - loss: 527.6747 - val_loss: 586.0836\n",
      "Epoch 31/50\n",
      "80000/80000 [==============================] - 2s 21us/sample - loss: 526.7973 - val_loss: 581.1134\n",
      "Epoch 32/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 524.0907 - val_loss: 589.0561\n",
      "Epoch 33/50\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 521.8068 - val_loss: 584.3490\n",
      "Epoch 34/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 520.8077 - val_loss: 584.6801\n",
      "Epoch 35/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 517.1422 - val_loss: 587.0650\n",
      "Epoch 36/50\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 516.1491 - val_loss: 591.9379\n",
      "Epoch 37/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 512.8856 - val_loss: 589.9189\n",
      "Epoch 38/50\n",
      "80000/80000 [==============================] - 2s 21us/sample - loss: 510.6670 - val_loss: 593.9495\n",
      "Epoch 39/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 507.6330 - val_loss: 598.4230\n",
      "Epoch 40/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 503.1888 - val_loss: 600.0013\n",
      "Epoch 41/50\n",
      "80000/80000 [==============================] - 2s 23us/sample - loss: 500.4336 - val_loss: 611.9426\n",
      "Epoch 42/50\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 497.6581 - val_loss: 606.4371\n",
      "Epoch 43/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 494.2765 - val_loss: 607.9915\n",
      "Epoch 44/50\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 490.5191 - val_loss: 608.1346\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 15us/sample - loss: 487.5843 - val_loss: 614.4159\n",
      "Epoch 46/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 484.0947 - val_loss: 614.3510\n",
      "Epoch 47/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 479.9792 - val_loss: 628.5563\n",
      "Epoch 48/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 476.0997 - val_loss: 626.4043\n",
      "Epoch 49/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 474.6953 - val_loss: 622.9332\n",
      "Epoch 50/50\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 470.3903 - val_loss: 628.0475\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "80000/80000 [==============================] - 4s 44us/sample - loss: 4323.5630 - val_loss: 602.9214\n",
      "Epoch 2/50\n",
      "80000/80000 [==============================] - 2s 28us/sample - loss: 1625.5582 - val_loss: 598.8293\n",
      "Epoch 3/50\n",
      "80000/80000 [==============================] - 2s 27us/sample - loss: 1590.4823 - val_loss: 615.2111\n",
      "Epoch 4/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 1566.2349 - val_loss: 580.1365\n",
      "Epoch 5/50\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 1547.4035 - val_loss: 587.9306\n",
      "Epoch 6/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1535.4869 - val_loss: 583.2309\n",
      "Epoch 7/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1528.9785 - val_loss: 600.3108\n",
      "Epoch 8/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1513.4258 - val_loss: 601.5193\n",
      "Epoch 9/50\n",
      "80000/80000 [==============================] - 2s 23us/sample - loss: 1503.1781 - val_loss: 618.8114\n",
      "Epoch 10/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1494.4231 - val_loss: 587.3843\n",
      "Epoch 11/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1488.9002 - val_loss: 608.9268\n",
      "Epoch 12/50\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 1482.1462 - val_loss: 598.5721\n",
      "Epoch 13/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1478.3516 - val_loss: 606.6443\n",
      "Epoch 14/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1466.8670 - val_loss: 584.2488\n",
      "Epoch 15/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1462.7890 - val_loss: 604.7641\n",
      "Epoch 16/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1452.4679 - val_loss: 620.0994\n",
      "Epoch 17/50\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 1452.4535 - val_loss: 591.2689\n",
      "Epoch 18/50\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 1448.9261 - val_loss: 598.2463\n",
      "Epoch 19/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1439.5103 - val_loss: 593.1461\n",
      "Epoch 20/50\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 1439.4237 - val_loss: 601.3086\n",
      "Epoch 21/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1431.0053 - val_loss: 614.9288\n",
      "Epoch 22/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1427.1000 - val_loss: 600.0105\n",
      "Epoch 23/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1422.5860 - val_loss: 620.6337\n",
      "Epoch 24/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1421.1658 - val_loss: 649.4040\n",
      "Epoch 25/50\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 1412.7618 - val_loss: 614.4649\n",
      "Epoch 26/50\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 1406.6330 - val_loss: 604.9511\n",
      "Epoch 27/50\n",
      "80000/80000 [==============================] - 2s 22us/sample - loss: 1398.8117 - val_loss: 612.1425\n",
      "Epoch 28/50\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 1397.7690 - val_loss: 622.2328\n",
      "Epoch 29/50\n",
      "80000/80000 [==============================] - 2s 21us/sample - loss: 1388.1460 - val_loss: 614.9006\n",
      "Epoch 30/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 1379.0802 - val_loss: 614.1665\n",
      "Epoch 31/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 1376.0800 - val_loss: 624.5626\n",
      "Epoch 32/50\n",
      "80000/80000 [==============================] - 2s 20us/sample - loss: 1367.0171 - val_loss: 622.0558\n",
      "Epoch 33/50\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 1357.8419 - val_loss: 614.5577\n",
      "Epoch 34/50\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 1346.9242 - val_loss: 627.7628\n",
      "Epoch 35/50\n",
      "77824/80000 [============================>.] - ETA: 0s - loss: 1340.1098"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "model.fit(x = xtrain, y = ytrain, epochs=epochs, batch_size=256, \n",
    "          validation_data=(xtest, ytest))\n",
    "\n",
    "model_class_weight.fit(x = xtrain, y = ytrain, epochs=epochs, batch_size=256, \n",
    "          validation_data=(xtest, ytest), class_weight=class_weights)\n",
    "\n",
    "relu_model.fit(x = xtrain.reshape(80000, 64), y = ytrain, epochs=epochs, batch_size=256, \n",
    "          validation_data=(xtest.reshape(20000, 64), ytest))\n",
    "\n",
    "relu_class_weigth_model.fit(x = xtrain.reshape(80000, 64), y = ytrain, epochs=epochs, batch_size=256, \n",
    "          validation_data=(xtest.reshape(20000, 64), ytest), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1 = sorted(np.absolute(ytest-model_class_weight.predict(xtest).reshape(ytest.shape)))\n",
    "x2 = sorted(np.absolute(ytest-model.predict(xtest).reshape(ytest.shape)))\n",
    "x3 = sorted(np.absolute(ytest-relu_class_weigth_model.predict(xtest.reshape(20000, 64)).reshape(ytest.shape)))\n",
    "x4 = sorted(np.absolute(ytest-relu_model.predict(xtest.reshape(20000, 64)).reshape(ytest.shape)))\n",
    "\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(ytest, bins=np.arange(10, 170, 5), label=\"3 random + 1 AI\")\n",
    "plt.hist(model.predict(xtest), bins=np.arange(10, 170, 5), label=\"prediction distribution\")\n",
    "plt.title(\"model without class wheighting - bins\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(ytest, bins=np.arange(10, 170, 5), label=\"3 random + 1 AI\")\n",
    "plt.hist(relu_model.predict(xtest.reshape(20000, 64)), bins=np.arange(10, 170, 5), label=\"prediction distribution\")\n",
    "plt.title(\" relu model without class wheighting - bins\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(ytest, bins=np.arange(10, 170, 5), label=\"3 random + 1 AI\")\n",
    "plt.hist(model_class_weight.predict(xtest), bins=np.arange(10, 170, 5), label=\"prediction distribution\")\n",
    "plt.title(\"model with class wheighting - bins\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(ytest, bins=np.arange(10, 170, 5), label=\"3 random + 1 AI\")\n",
    "plt.hist(relu_class_weigth_model.predict(xtest.reshape(20000, 64)), bins=np.arange(10, 170, 5), label=\"prediction distribution\")\n",
    "plt.title(\" relu model with class wheighting - bins\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(histtype='step', bins=np.arange(0, 80, 10))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(x1, label = \"cnn cw\", **kwargs)\n",
    "plt.hist(x2,label = \"cnn\", **kwargs)\n",
    "plt.hist(x3,label = \"relu cw\", **kwargs)\n",
    "plt.hist(x4,label = \"relu\", **kwargs)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gym-coinche]",
   "language": "python",
   "name": "conda-env-gym-coinche-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
